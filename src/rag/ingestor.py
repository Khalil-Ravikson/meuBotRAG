"""
rag/ingestor.py ‚Äî Ingest√£o de PDFs e TXTs no banco vetorial
============================================================
Extra√≠do de rag_service.py (m√©todo ingerir_base_conhecimento).

Responsabilidades:
  - Varrer DATA_DIR em busca de PDFs e TXTs
  - Parsear PDFs com LlamaParse (instru√ß√£o espec√≠fica por arquivo)
  - Ler TXTs diretamente (sem LlamaParse)
  - Fazer chunking com RecursiveCharacterTextSplitter
  - Salvar chunks no pgvector com metadado 'source' correto
  - Verificar se o banco j√° est√° populado (evita re-ingest√£o)

NOTA SOBRE OS NOMES DE ARQUIVO:
  O metadado 'source' salvo no banco DEVE bater EXATAMENTE com SOURCE_*
  nas tools. Use diagnose_banco() ap√≥s a ingest√£o para confirmar.

CORRE√á√ÉO IMPORTANTE vs rag_service.py:
  O c√≥digo original tinha uma vari√°vel 'arquivos_pdf' que inclu√≠a TXTs
  mas a vari√°vel usada na itera√ß√£o era 'arquivos_pdf' (apenas PDFs).
  Aqui separamos corretamente: PDFs usam LlamaParse, TXTs s√£o lidos direto.
"""
from __future__ import annotations
import os
import re
import glob
import logging

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from llama_parse import LlamaParse

from src.rag.vector_store import get_vector_store, diagnosticar
from src.infrastructure.settings import settings

logger = logging.getLogger(__name__)

# =============================================================================
# Configura√ß√£o por arquivo (fonte √∫nica da verdade para parsing e chunking)
# =============================================================================
# ATEN√á√ÉO: as chaves aqui s√£o os NOMES EXATOS dos arquivos em DATA_DIR.
# O metadado 'source' salvo no banco ser√° exatamente esse nome.
# Ele deve bater com SOURCE_* nas tools.

PDF_CONFIG: dict[str, dict] = {
    # ‚îÄ‚îÄ PDFs (processados via LlamaParse) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    "calendario-academico-2026.pdf": {
        "chunk_size":   400,
        "chunk_overlap": 50,
        "parsing_instruction": (
            "Este PDF √© o Calend√°rio Acad√™mico da UEMA 2026. "
            "Para CADA linha de evento na tabela, formate assim:\n"
            "EVENTO: [nome do evento] | DATA: [data ou per√≠odo] | SEM: [semestre]\n"
            "Exemplo: EVENTO: Matr√≠cula de veteranos | DATA: 03/02/2026 a 07/02/2026 | SEM: 2026.1\n"
            "Mantenha TODOS os eventos e datas."
        ),
    },
    "edital_paes_2026.pdf": {
        "chunk_size":   600,
        "chunk_overlap": 80,
        "parsing_instruction": (
            "Este PDF √© o Edital do Processo Seletivo PAES 2026 da UEMA. "
            "Para tabelas de vagas, preserve:\n"
            "CURSO: [nome] | TURNO: [turno] | AC: [n¬∫] | PcD: [n¬∫] | TOTAL: [n¬∫]\n"
            "Para categorias de cotas:\n"
            "CATEGORIA: [sigla] | NOME: [nome completo] | P√öBLICO: [descri√ß√£o]\n"
            "Preserve todos os n√∫meros de vagas e numera√ß√£o dos itens."
        ),
    },
    "guia_contatos_2025.pdf": {
        "chunk_size":   300,
        "chunk_overlap": 30,
        "parsing_instruction": (
            "Este PDF √© o Guia de Contatos da UEMA 2025. "
            "Para cada linha de contato, formate:\n"
            "CARGO: [cargo] | NOME: [nome completo] | EMAIL: [email] | TEL: [telefone]\n"
            "Exemplo: CARGO: Diretor CECEN | NOME: Regina C√©lia | EMAIL: cecen@uema.br | TEL: (98) 99232-4837\n"
            "Mantenha o nome do centro/unidade como cabe√ßalho de cada bloco."
        ),
    },

    # ‚îÄ‚îÄ TXTs (lidos diretamente, sem LlamaParse) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    "contatos_saoluis.txt": {
        "chunk_size":   300,
        "chunk_overlap": 30,
        "parsing_instruction": None,
    },
    "regras_ru.txt": {
        "chunk_size":   400,
        "chunk_overlap": 50,
        "parsing_instruction": None,
    },
}


class Ingestor:
    """
    Singleton de ingest√£o.
    Instancie uma vez e reutilize ‚Äî evita re-carregar o modelo de embedding.
    """

    def __init__(self):
        self._vs = get_vector_store()

    # =========================================================================
    # API p√∫blica
    # =========================================================================

    def ingerir_se_necessario(self) -> None:
        """
        Verifica se o banco j√° est√° populado.
        Se n√£o estiver, ingere todos os arquivos de DATA_DIR.
        """
        if self._banco_populado():
            logger.info("üíæ Banco vetorial j√° populado. Pulando ingest√£o.")
            return
        self.ingerir_tudo()

    def ingerir_tudo(self) -> None:
        """
        For√ßa a ingest√£o de todos os arquivos, mesmo se o banco n√£o estiver vazio.
        Use ao atualizar PDFs.
        """
        data_dir = settings.DATA_DIR
        logger.info("üïµÔ∏è  Iniciando ingest√£o em: %s", data_dir)

        arquivos = self._listar_arquivos(data_dir)
        if not arquivos:
            logger.warning("‚ö†Ô∏è  Nenhum arquivo encontrado em %s", data_dir)
            return

        logger.info("üìÅ Arquivos encontrados: %s", [os.path.basename(a) for a in arquivos])

        for arquivo in arquivos:
            self._ingerir_arquivo(arquivo)

        logger.info("‚úÖ Ingest√£o conclu√≠da.")
        self.diagnosticar()

    def diagnosticar(self) -> set[str]:
        """Retorna e loga os sources presentes no banco."""
        sources = diagnosticar()
        print("=" * 60)
        print("üîç DIAGN√ìSTICO DO BANCO VETORIAL")
        print(f"   Sources presentes: {sources}")
        print(f"   Esperados (PDF_CONFIG): {list(PDF_CONFIG.keys())}")
        faltam = set(PDF_CONFIG.keys()) - sources
        if faltam:
            print(f"   ‚ùå N√ÉO INGERIDOS: {faltam}")
        else:
            print("   ‚úÖ Todos os arquivos est√£o no banco.")
        print("=" * 60)
        return sources

    # =========================================================================
    # Internos
    # =========================================================================

    def _banco_populado(self) -> bool:
        try:
            docs = self._vs.similarity_search("UEMA 2026", k=1)
            if docs:
                return True
        except Exception as e:
            logger.warning("‚ö†Ô∏è  similarity_search falhou: %s", e)
        try:
            if self._vs._collection.count() > 0:
                return True
        except Exception:
            pass
        return False

    def _listar_arquivos(self, data_dir: str) -> list[str]:
        """Retorna PDFs e TXTs da pasta, ordenados."""
        pdfs = glob.glob(os.path.join(data_dir, "*.[pP][dD][fF]"))
        txts = glob.glob(os.path.join(data_dir, "*.[tT][xX][tT]"))
        return sorted(pdfs + txts)

    def _ingerir_arquivo(self, arquivo: str) -> None:
        nome   = os.path.basename(arquivo)
        config = PDF_CONFIG.get(nome)

        if not config:
            logger.warning("‚ö†Ô∏è  '%s' n√£o est√° no PDF_CONFIG. Pulando.", nome)
            logger.warning("   Esperados: %s", list(PDF_CONFIG.keys()))
            return

        logger.info("üì¶ Processando '%s'...", nome)
        eh_txt = nome.lower().endswith(".txt")

        try:
            documentos = (
                self._ler_txt(arquivo, nome)
                if eh_txt
                else self._parsear_pdf(arquivo, nome, config["parsing_instruction"])
            )

            if not documentos:
                logger.warning("‚ö†Ô∏è  Nenhum conte√∫do extra√≠do de '%s'.", nome)
                return

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=config["chunk_size"],
                chunk_overlap=config["chunk_overlap"],
                separators=["\n\n", "\n", " ", ""],
            )
            chunks = splitter.split_documents(documentos)
            self._vs.add_documents(chunks)
            logger.info("‚úÖ '%s': %d chunks salvos.", nome, len(chunks))

        except Exception as e:
            logger.exception("‚ùå Erro ao ingerir '%s': %s", nome, e)

    def _ler_txt(self, arquivo: str, nome: str) -> list[Document]:
        """L√™ TXT diretamente, sem LlamaParse."""
        with open(arquivo, "r", encoding="utf-8") as f:
            texto = _limpar_texto(f.read())
        if not texto:
            return []
        return [Document(page_content=texto, metadata={"source": nome})]

    def _parsear_pdf(
        self, arquivo: str, nome: str, instrucao: str | None
    ) -> list[Document]:
        """Usa LlamaParse para parsear o PDF com instru√ß√£o espec√≠fica."""
        parser = LlamaParse(
            api_key=settings.LLAMA_CLOUD_API_KEY,
            result_type="markdown",
            language="pt",
            verbose=False,
            parsing_instruction=instrucao or "",
        )
        llama_docs = parser.load_data(arquivo)
        documentos: list[Document] = []

        for llama_doc in llama_docs:
            texto = _limpar_texto(llama_doc.text)
            if not texto:
                continue
            documentos.append(Document(
                page_content=texto,
                metadata={
                    "source": nome,
                    # Preserva metadados escalares do LlamaParse (page_number, etc.)
                    **{
                        k: v
                        for k, v in (llama_doc.metadata or {}).items()
                        if isinstance(v, (str, int, float, bool))
                    },
                },
            ))

        return documentos


# =============================================================================
# Utilit√°rios
# =============================================================================

def _limpar_texto(text: str) -> str:
    """Remove ru√≠do visual comum em PDFs da UEMA."""
    if not text:
        return ""
    # Remove linhas de tabela com s√≥ pipes e n√∫meros
    text = re.sub(r"^\|[\s\d\|\-:]+\|$", "", text, flags=re.MULTILINE)
    # Remove cabe√ßalhos repetitivos
    text = re.sub(
        r"UNIVERSIDADE ESTADUAL DO MARANH√ÉO|www\.uema\.br|UEMA\s*[-‚Äì]\s*Campus",
        "", text, flags=re.IGNORECASE,
    )
    # Remove linhas em branco excessivas
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()